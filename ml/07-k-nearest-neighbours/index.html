<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>07 - K-Nearest Neighbours | ML Tuts</title>
<meta name="keywords" content="">
<meta name="description" content="Intro Are you interested in machine learning and classification? The K-Nearest Neighbours algorithm, or KNN for short, is a simple yet effective algorithm used for classification tasks. In this blog post, we&rsquo;ll dive into KNN&rsquo;s implementation in Python, specifically in the scikit-learn library. We&rsquo;ll cover the necessary modules used, mounting Google Drive in our virtual machine, training and testing the model, calculating its accuracy, making predictions, interpreting confusion matrix values, and using other metrics such as precision and recall.">
<meta name="author" content="">
<link rel="canonical" href="http://mltuts.online/ml/07-k-nearest-neighbours/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css" integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe&#43;FVUFzPh7U=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="http://mltuts.online/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://mltuts.online/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://mltuts.online/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://mltuts.online/apple-touch-icon.png">
<link rel="mask-icon" href="http://mltuts.online/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="07 - K-Nearest Neighbours" />
<meta property="og:description" content="Intro Are you interested in machine learning and classification? The K-Nearest Neighbours algorithm, or KNN for short, is a simple yet effective algorithm used for classification tasks. In this blog post, we&rsquo;ll dive into KNN&rsquo;s implementation in Python, specifically in the scikit-learn library. We&rsquo;ll cover the necessary modules used, mounting Google Drive in our virtual machine, training and testing the model, calculating its accuracy, making predictions, interpreting confusion matrix values, and using other metrics such as precision and recall." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://mltuts.online/ml/07-k-nearest-neighbours/" /><meta property="article:section" content="ML" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="07 - K-Nearest Neighbours"/>
<meta name="twitter:description" content="Intro Are you interested in machine learning and classification? The K-Nearest Neighbours algorithm, or KNN for short, is a simple yet effective algorithm used for classification tasks. In this blog post, we&rsquo;ll dive into KNN&rsquo;s implementation in Python, specifically in the scikit-learn library. We&rsquo;ll cover the necessary modules used, mounting Google Drive in our virtual machine, training and testing the model, calculating its accuracy, making predictions, interpreting confusion matrix values, and using other metrics such as precision and recall."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "MLs",
      "item": "http://mltuts.online/ml/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "07 - K-Nearest Neighbours",
      "item": "http://mltuts.online/ml/07-k-nearest-neighbours/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "07 - K-Nearest Neighbours",
  "name": "07 - K-Nearest Neighbours",
  "description": "Intro Are you interested in machine learning and classification? The K-Nearest Neighbours algorithm, or KNN for short, is a simple yet effective algorithm used for classification tasks. In this blog post, we\u0026rsquo;ll dive into KNN\u0026rsquo;s implementation in Python, specifically in the scikit-learn library. We\u0026rsquo;ll cover the necessary modules used, mounting Google Drive in our virtual machine, training and testing the model, calculating its accuracy, making predictions, interpreting confusion matrix values, and using other metrics such as precision and recall.",
  "keywords": [
    
  ],
  "articleBody": "Intro Are you interested in machine learning and classification? The K-Nearest Neighbours algorithm, or KNN for short, is a simple yet effective algorithm used for classification tasks. In this blog post, we’ll dive into KNN’s implementation in Python, specifically in the scikit-learn library. We’ll cover the necessary modules used, mounting Google Drive in our virtual machine, training and testing the model, calculating its accuracy, making predictions, interpreting confusion matrix values, and using other metrics such as precision and recall. Whether you’re a beginner or an experienced machine learning practitioner, this post will give you a solid understanding of KNN and how it can be used for classification tasks.\nImportant Code Snippets Mounting Google Drive in your VM Downloading files to your local file system Open files from your local file system Showing CV2 Images Modules Used from google.colab import drive # for mounting gDrive in our VM from sklearn import datasets # for loading dataset from sklearn.model_selection import train_test_split # for split train test from sklearn.neighbors import KNeighborsClassifier # K-Nearest neighbours KNN from sklearn.metrics import confusion_matrix # for confustion matrix from sklearn.metrics import precision_score, recall_score, average_precision_score, f1_score from sklearn.metrics import classification_report import matplotlib.pyplot as plt Mounting gDrive In Our VM # mounting gDrive in our VM drive.mount('/gdrive') Train, Accuracy, Prediction, Confusion Matrix For K-Nearest Neighbours # load dataset iris = datasets.load_iris() iris.keys() # everything that dataset hold iris.target # classes iris.target_names # classes name iris.data # data iris.feature_names # features names x = iris.data y = iris.target # train test split # splitting x on 50% 50% and y on 50% 50% X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.5, random_state = 42) # X -\u003e data and Y -\u003e target # train model modelKNN = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train) # n_neighbours which 3 neighbours model is looking modelKNN.classes_ # we can see what classes we have in there modelKNN.effective_metric_ # display what effectiv metric we use for calculating nearest neighbours # accuracy print(f\"Accuracy: {modelKNN.score(X_test, y_test)}\") # prediction y_predict_KNN = modelKNN.predict(X_test) classes = {0: 'setosa', 1: 'versicolor', 2: 'virginica'} # we give name to our classes print(f\"For first flower from test set prediction is class: {classes[y_predict_KNN[0]]}, real class is: {classes[y_test[0]]}\") # confusion matrix: print(f\"Values from confusion matrix: \\n{confusion_matrix(y_test, y_predict_KNN)}\") # prouči output Interpret Confustion Matrix Values This is a Multi-Class Confusion Matrix where rows are PREDICTED and where collumns are ACTUAL For example:\n[[29 0 0] [ 0 23 0] [ 0 2 21]] Interpretation:\n# setosa, versicolor, virginica [[29 0 0] # setosa [ 0 23 0] # versicolor [ 0 2 21]] # virginica For setosa model predicted all of them For versicolor model predicted all of them For virginica model predicted 21 but missed 2 samples of versicolor A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.\nIn this example, the confusion matrix has three rows and three columns representing the actual and predicted class labels. The rows represent the actual or ground truth classes, and the columns represent the predicted classes.\nThe first row and column are for the first class, where 29 samples were correctly predicted as the first class, and there were no false positives or false negatives.\nThe second row and column are for the second class, where 23 samples were correctly predicted as the second class, and there were no false positives or false negatives.\nThe third row and column are for the third class, where 21 samples were correctly predicted as the third class, and there were two false positives (samples that were predicted as the third class, but actually belong to the first or second class).\nOther Metrics And Classification Report # other metrics: print(precision_score(y_test, y_predict_KNN, average = None)) # precision score print(recall_score(y_test, y_predict_KNN, average = None)) # recall score print(f1_score(y_test, y_predict_KNN, average = None)) # f1 score print(classification_report(y_test, y_predict_KNN, target_names=iris.target_names)) # all that you need for metrics Precision and Recall There is often a trade-off between precision and recall, meaning that optimizing one metric can negatively affect the other. For example, if a model becomes more conservative in its predictions and only predicts positive when it is very confident, it may increase its precision but lower its recall. On the other hand, if a model becomes more liberal in its predictions and predicts more positives, it may increase its recall but lower its precision.\nConfusion Matrix - Binary In the context of binary classification, a confusion matrix is a table that is used to evaluate the performance of a classification model. The matrix displays the number of actual positive and negative samples, and the number of samples that are predicted as positive and negative by the model. The four values in the confusion matrix are:\nTrue Positive (TP): the number of samples that are actually positive and are correctly predicted as positive by the model. False Positive (FP): the number of samples that are actually negative but are incorrectly predicted as positive by the model. True Negative (TN): the number of samples that are actually negative and are correctly predicted as negative by the model. False Negative (FN): the number of samples that are actually positive but are incorrectly predicted as negative by the model. The confusion matrix helps in evaluating the performance of a binary classification model by comparing the predicted output with the actual output. It provides a more detailed picture of the performance of a model than just looking at overall accuracy. The accuracy of a model can be misleading in situations where there is a class imbalance, and the majority class can have a high accuracy even when the model is not performing well on the minority class. In such situations, the precision and recall values can be more informative.\nPrecision: Precision measures the proportion of true positives among the total samples that the model has predicted as positive. It is calculated as TP / (TP + FP). A high precision indicates that the model is making fewer false positive predictions. Recall: Recall measures the proportion of true positives among the actual positive samples. It is calculated as TP / (TP + FN). A high recall indicates that the model is correctly identifying more positive samples. Actual Positive Actual Negative Predicted Positive True Positive False Positive Predicted Negative False Negative True Negative Extra - Finding Model With Best Accuracy # Define empty lists to store model accuracies and number of neighbors differentModels = [] numberOfNeighbours = [] # Loop through a range of values for number of neighbors for i in range(1, 21): # Create a KNN model with i neighbors and fit it to the training data modelKNN = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train) # Append the accuracy score and number of neighbors to their respective lists differentModels.append(modelKNN.score(X_test, y_test)) numberOfNeighbours.append(i) # Plot the different accuracy scores for each number of neighbors plt.plot(range(1, 21), differentModels) plt.show() # Find the max accuracy and print the corresponding number of neighbors maxAccuracy = max(differentModels) print(\"Model with the highest accuracy:\") for i in range(len(differentModels)): if maxAccuracy == differentModels[i]: print(f\"Maximum accuracy: {maxAccuracy}, number of neighbors: {numberOfNeighbours[i]}\") ",
  "wordCount" : "1185",
  "inLanguage": "en",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://mltuts.online/ml/07-k-nearest-neighbours/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ML Tuts",
    "logo": {
      "@type": "ImageObject",
      "url": "http://mltuts.online/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://mltuts.online/" accesskey="h" title="ML Tuts (Alt + H)">ML Tuts</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://mltuts.online/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://mltuts.online/ml/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://mltuts.online/menu/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://mltuts.online/menu/contact/" title="Contact">
                    <span>Contact</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://mltuts.online/">Home</a>&nbsp;»&nbsp;<a href="http://mltuts.online/ml/">MLs</a></div>
    <h1 class="post-title">
      07 - K-Nearest Neighbours
    </h1>
    <div class="post-meta">6 min

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intro" aria-label="Intro">Intro</a><ul>
                        
                <li>
                    <a href="#important-code-snippets" aria-label="Important Code Snippets">Important Code Snippets</a></li>
                <li>
                    <a href="#modules-used" aria-label="Modules Used">Modules Used</a></li>
                <li>
                    <a href="#mounting-gdrive-in-our-vm" aria-label="Mounting gDrive In Our VM">Mounting gDrive In Our VM</a></li>
                <li>
                    <a href="#train-accuracy-prediction-confusion-matrix-for-k-nearest-neighbours" aria-label="Train, Accuracy, Prediction, Confusion Matrix For K-Nearest Neighbours">Train, Accuracy, Prediction, Confusion Matrix For K-Nearest Neighbours</a></li>
                <li>
                    <a href="#interpret-confustion-matrix-values" aria-label="Interpret Confustion Matrix Values">Interpret Confustion Matrix Values</a></li>
                <li>
                    <a href="#other-metrics-and-classification-report" aria-label="Other Metrics And Classification Report">Other Metrics And Classification Report</a><ul>
                        
                <li>
                    <a href="#precision-and-recall" aria-label="Precision and Recall">Precision and Recall</a></li>
                <li>
                    <a href="#confusion-matrix---binary" aria-label="Confusion Matrix - Binary">Confusion Matrix - Binary</a></li></ul>
                </li>
                <li>
                    <a href="#extra---finding-model-with-best-accuracy" aria-label="Extra - Finding Model With Best Accuracy">Extra - Finding Model With Best Accuracy</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="intro">Intro<a hidden class="anchor" aria-hidden="true" href="#intro">#</a></h1>
<p>Are you interested in machine learning and classification? The K-Nearest Neighbours algorithm, or KNN for short, is a simple yet effective algorithm used for classification tasks. In this blog post, we&rsquo;ll dive into KNN&rsquo;s implementation in Python, specifically in the scikit-learn library. We&rsquo;ll cover the necessary modules used, mounting Google Drive in our virtual machine, training and testing the model, calculating its accuracy, making predictions, interpreting confusion matrix values, and using other metrics such as precision and recall. Whether you&rsquo;re a beginner or an experienced machine learning practitioner, this post will give you a solid understanding of KNN and how it can be used for classification tasks.</p>
<h2 id="important-code-snippets">Important Code Snippets<a hidden class="anchor" aria-hidden="true" href="#important-code-snippets">#</a></h2>
<ul>
<li>Mounting Google Drive in your VM</li>
<li>Downloading files to your local file system</li>
<li>Open files from your local file system</li>
<li>Showing CV2 Images</li>
</ul>
<h2 id="modules-used">Modules Used<a hidden class="anchor" aria-hidden="true" href="#modules-used">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span>from<span style="color:#960050;background-color:#1e0010"> </span>google<span style="color:#f92672">.</span>colab<span style="color:#960050;background-color:#1e0010"> </span>import<span style="color:#960050;background-color:#1e0010"> </span>drive <span style="color:#75715e"># for mounting gDrive in our VM</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn <span style="color:#f92672">import</span> datasets <span style="color:#75715e"># for loading dataset</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split <span style="color:#75715e"># for split train test  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neighbors <span style="color:#f92672">import</span> KNeighborsClassifier <span style="color:#75715e"># K-Nearest neighbours KNN  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix <span style="color:#75715e"># for confustion matrix</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> precision_score, recall_score, average_precision_score, f1_score  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> classification_report  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span></code></pre></div><h2 id="mounting-gdrive-in-our-vm">Mounting gDrive In Our VM<a hidden class="anchor" aria-hidden="true" href="#mounting-gdrive-in-our-vm">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># mounting gDrive in our VM  </span>
</span></span><span style="display:flex;"><span>drive<span style="color:#f92672">.</span>mount(<span style="color:#e6db74">&#39;/gdrive&#39;</span>)
</span></span></code></pre></div><h2 id="train-accuracy-prediction-confusion-matrix-for-k-nearest-neighbours">Train, Accuracy, Prediction, Confusion Matrix For K-Nearest Neighbours<a hidden class="anchor" aria-hidden="true" href="#train-accuracy-prediction-confusion-matrix-for-k-nearest-neighbours">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># load dataset  </span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>load_iris()   
</span></span><span style="display:flex;"><span>iris<span style="color:#f92672">.</span>keys() <span style="color:#75715e"># everything that dataset hold  </span>
</span></span><span style="display:flex;"><span>iris<span style="color:#f92672">.</span>target <span style="color:#75715e"># classes  </span>
</span></span><span style="display:flex;"><span>iris<span style="color:#f92672">.</span>target_names <span style="color:#75715e"># classes name  </span>
</span></span><span style="display:flex;"><span>iris<span style="color:#f92672">.</span>data <span style="color:#75715e"># data  </span>
</span></span><span style="display:flex;"><span>iris<span style="color:#f92672">.</span>feature_names <span style="color:#75715e"># features names  </span>
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># train test split  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># splitting x on 50% 50% and y on 50% 50%  </span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(x, y, test_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, random_state <span style="color:#f92672">=</span> <span style="color:#ae81ff">42</span>) <span style="color:#75715e"># X -&gt; data and Y -&gt; target  </span>
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># train model  </span>
</span></span><span style="display:flex;"><span>modelKNN <span style="color:#f92672">=</span> KNeighborsClassifier(n_neighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)<span style="color:#f92672">.</span>fit(X_train, y_train) <span style="color:#75715e"># n_neighbours which 3 neighbours model is looking  </span>
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>modelKNN<span style="color:#f92672">.</span>classes_ <span style="color:#75715e"># we can see what classes we have in there  </span>
</span></span><span style="display:flex;"><span>modelKNN<span style="color:#f92672">.</span>effective_metric_ <span style="color:#75715e"># display what effectiv metric we use for calculating nearest neighbours  </span>
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># accuracy  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Accuracy: </span><span style="color:#e6db74">{</span>modelKNN<span style="color:#f92672">.</span>score(X_test, y_test)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># prediction  </span>
</span></span><span style="display:flex;"><span>y_predict_KNN <span style="color:#f92672">=</span> modelKNN<span style="color:#f92672">.</span>predict(X_test)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>classes <span style="color:#f92672">=</span> {<span style="color:#ae81ff">0</span>: <span style="color:#e6db74">&#39;setosa&#39;</span>, <span style="color:#ae81ff">1</span>: <span style="color:#e6db74">&#39;versicolor&#39;</span>, <span style="color:#ae81ff">2</span>: <span style="color:#e6db74">&#39;virginica&#39;</span>} <span style="color:#75715e"># we give name to our classes  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;For first flower from test set prediction is class: </span><span style="color:#e6db74">{</span>classes[y_predict_KNN[<span style="color:#ae81ff">0</span>]]<span style="color:#e6db74">}</span><span style="color:#e6db74">, real class is: </span><span style="color:#e6db74">{</span>classes[y_test[<span style="color:#ae81ff">0</span>]]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># confusion matrix:  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Values from confusion matrix: </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>confusion_matrix(y_test, y_predict_KNN)<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>) <span style="color:#75715e"># prouči output</span>
</span></span></code></pre></div><h2 id="interpret-confustion-matrix-values">Interpret Confustion Matrix Values<a hidden class="anchor" aria-hidden="true" href="#interpret-confustion-matrix-values">#</a></h2>
<p><strong>This is a Multi-Class Confusion Matrix</strong> where rows are <strong>PREDICTED</strong> and where collumns are <strong>ACTUAL</strong>
For example:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Shell" data-lang="Shell"><span style="display:flex;"><span><span style="color:#f92672">[[</span><span style="color:#ae81ff">29</span>  <span style="color:#ae81ff">0</span>  0<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span> <span style="color:#f92672">[</span> <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">23</span>  0<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span> <span style="color:#f92672">[</span> <span style="color:#ae81ff">0</span>  <span style="color:#ae81ff">2</span> 21<span style="color:#f92672">]]</span>
</span></span></code></pre></div><p>Interpretation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Shell" data-lang="Shell"><span style="display:flex;"><span><span style="color:#75715e"># setosa, versicolor, virginica</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[[</span><span style="color:#ae81ff">29</span>  <span style="color:#ae81ff">0</span>  0<span style="color:#f92672">]</span> <span style="color:#75715e"># setosa</span>
</span></span><span style="display:flex;"><span> <span style="color:#f92672">[</span> <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">23</span>  0<span style="color:#f92672">]</span> <span style="color:#75715e"># versicolor</span>
</span></span><span style="display:flex;"><span> <span style="color:#f92672">[</span> <span style="color:#ae81ff">0</span>  <span style="color:#ae81ff">2</span> 21<span style="color:#f92672">]]</span> <span style="color:#75715e"># virginica</span>
</span></span></code></pre></div><ul>
<li>For setosa model predicted all of them</li>
<li>For versicolor model predicted all of them</li>
<li>For virginica model predicted 21 but missed 2 samples of versicolor</li>
</ul>
<p>A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.</p>
<p>In this example, the confusion matrix has three rows and three columns representing the actual and predicted class labels. The rows represent the actual or ground truth classes, and the columns represent the predicted classes.</p>
<ul>
<li>
<p>The first row and column are for the first class, where 29 samples were correctly predicted as the first class, and there were no false positives or false negatives.</p>
</li>
<li>
<p>The second row and column are for the second class, where 23 samples were correctly predicted as the second class, and there were no false positives or false negatives.</p>
</li>
<li>
<p>The third row and column are for the third class, where 21 samples were correctly predicted as the third class, and there were two false positives (samples that were predicted as the third class, but actually belong to the first or second class).</p>
</li>
</ul>
<h2 id="other-metrics-and-classification-report">Other Metrics And Classification Report<a hidden class="anchor" aria-hidden="true" href="#other-metrics-and-classification-report">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># other metrics:  </span>
</span></span><span style="display:flex;"><span>print(precision_score(y_test, y_predict_KNN, average <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>)) <span style="color:#75715e"># precision score  </span>
</span></span><span style="display:flex;"><span>print(recall_score(y_test, y_predict_KNN, average <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>)) <span style="color:#75715e"># recall score  </span>
</span></span><span style="display:flex;"><span>print(f1_score(y_test, y_predict_KNN, average <span style="color:#f92672">=</span> <span style="color:#66d9ef">None</span>)) <span style="color:#75715e"># f1 score</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(classification_report(y_test,<span style="color:#960050;background-color:#1e0010"> </span>y_predict_KNN,<span style="color:#960050;background-color:#1e0010"> </span>target_names<span style="color:#f92672">=</span>iris<span style="color:#f92672">.</span>target_names))<span style="color:#960050;background-color:#1e0010"> </span><span style="color:#75715e"># all that you need for metrics</span>
</span></span></code></pre></div><h3 id="precision-and-recall">Precision and Recall<a hidden class="anchor" aria-hidden="true" href="#precision-and-recall">#</a></h3>
<p>There is often a trade-off between precision and recall, meaning that optimizing one metric can negatively affect the other. For example, if a model becomes more conservative in its predictions and only predicts positive when it is very confident, it may increase its precision but lower its recall. On the other hand, if a model becomes more liberal in its predictions and predicts more positives, it may increase its recall but lower its precision.</p>
<h3 id="confusion-matrix---binary">Confusion Matrix - Binary<a hidden class="anchor" aria-hidden="true" href="#confusion-matrix---binary">#</a></h3>
<p>In the context of binary classification, a confusion matrix is a table that is used to evaluate the performance of a classification model. The matrix displays the number of actual positive and negative samples, and the number of samples that are predicted as positive and negative by the model. The four values in the confusion matrix are:</p>
<ul>
<li>True Positive (TP): the number of samples that are actually positive and are correctly predicted as positive by the model.</li>
<li>False Positive (FP): the number of samples that are actually negative but are incorrectly predicted as positive by the model.</li>
<li>True Negative (TN): the number of samples that are actually negative and are correctly predicted as negative by the model.</li>
<li>False Negative (FN): the number of samples that are actually positive but are incorrectly predicted as negative by the model.</li>
</ul>
<p>The confusion matrix helps in evaluating the performance of a binary classification model by comparing the predicted output with the actual output. It provides a more detailed picture of the performance of a model than just looking at overall accuracy. The accuracy of a model can be misleading in situations where there is a class imbalance, and the majority class can have a high accuracy even when the model is not performing well on the minority class. In such situations, the precision and recall values can be more informative.</p>
<ul>
<li>Precision: Precision measures the proportion of true positives among the total samples that the model has predicted as positive. It is calculated as TP / (TP + FP). A high precision indicates that the model is making fewer false positive predictions.</li>
<li>Recall: Recall measures the proportion of true positives among the actual positive samples. It is calculated as TP / (TP + FN). A high recall indicates that the model is correctly identifying more positive samples.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">Actual Positive</th>
<th style="text-align:center">Actual Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Predicted Positive</strong></td>
<td style="text-align:center">True Positive</td>
<td style="text-align:center">False Positive</td>
</tr>
<tr>
<td style="text-align:center"><strong>Predicted Negative</strong></td>
<td style="text-align:center">False Negative</td>
<td style="text-align:center">True Negative</td>
</tr>
</tbody>
</table>
<h2 id="extra---finding-model-with-best-accuracy">Extra - Finding Model With Best Accuracy<a hidden class="anchor" aria-hidden="true" href="#extra---finding-model-with-best-accuracy">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#75715e"># Define empty lists to store model accuracies and number of neighbors  </span>
</span></span><span style="display:flex;"><span>differentModels <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span>numberOfNeighbours <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loop through a range of values for number of neighbors  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">21</span>):  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Create a KNN model with i neighbors and fit it to the training data  </span>
</span></span><span style="display:flex;"><span>    modelKNN <span style="color:#f92672">=</span> KNeighborsClassifier(n_neighbors<span style="color:#f92672">=</span>i)<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Append the accuracy score and number of neighbors to their respective lists  </span>
</span></span><span style="display:flex;"><span>    differentModels<span style="color:#f92672">.</span>append(modelKNN<span style="color:#f92672">.</span>score(X_test, y_test))  
</span></span><span style="display:flex;"><span>    numberOfNeighbours<span style="color:#f92672">.</span>append(i)  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot the different accuracy scores for each number of neighbors  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">21</span>), differentModels)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Find the max accuracy and print the corresponding number of neighbors  </span>
</span></span><span style="display:flex;"><span>maxAccuracy <span style="color:#f92672">=</span> max(differentModels)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Model with the highest accuracy:&#34;</span>)  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(differentModels)):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> maxAccuracy <span style="color:#f92672">==</span> differentModels[i]:  
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Maximum accuracy: </span><span style="color:#e6db74">{</span>maxAccuracy<span style="color:#e6db74">}</span><span style="color:#e6db74">, number of neighbors: </span><span style="color:#e6db74">{</span>numberOfNeighbours[i]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 07 - K-Nearest Neighbours on twitter"
        href="https://twitter.com/intent/tweet/?text=07%20-%20K-Nearest%20Neighbours&amp;url=http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 07 - K-Nearest Neighbours on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f&amp;title=07%20-%20K-Nearest%20Neighbours&amp;summary=07%20-%20K-Nearest%20Neighbours&amp;source=http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 07 - K-Nearest Neighbours on reddit"
        href="https://reddit.com/submit?url=http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f&title=07%20-%20K-Nearest%20Neighbours">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 07 - K-Nearest Neighbours on facebook"
        href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 07 - K-Nearest Neighbours on whatsapp"
        href="https://api.whatsapp.com/send?text=07%20-%20K-Nearest%20Neighbours%20-%20http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 07 - K-Nearest Neighbours on telegram"
        href="https://telegram.me/share/url?text=07%20-%20K-Nearest%20Neighbours&amp;url=http%3a%2f%2fmltuts.online%2fml%2f07-k-nearest-neighbours%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="http://mltuts.online/">ML Tuts</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
